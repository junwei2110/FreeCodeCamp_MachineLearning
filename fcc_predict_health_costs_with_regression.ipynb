{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fcc_predict_health_costs_with_regression.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junwei2110/FreeCodeCamp_MachineLearning/blob/Linear-DNN-Regression-Health-Costs/fcc_predict_health_costs_with_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9TX15KOkPBV"
      },
      "source": [
        "*Note: You are currently reading this using Google Colaboratory which is a cloud-hosted version of Jupyter Notebook. This is a document containing both text cells for documentation and runnable code cells. If you are unfamiliar with Jupyter Notebook, watch this 3-minute introduction before starting this challenge: https://www.youtube.com/watch?v=inN8seMm7UI*\n",
        "\n",
        "---\n",
        "\n",
        "In this challenge, you will predict healthcare costs using a regression algorithm.\n",
        "\n",
        "You are given a dataset that contains information about different people including their healthcare costs. Use the data to predict healthcare costs based on new data.\n",
        "\n",
        "The first two cells of this notebook import libraries and the data.\n",
        "\n",
        "Make sure to convert categorical data to numbers. Use 80% of the data as the `train_dataset` and 20% of the data as the `test_dataset`.\n",
        "\n",
        "`pop` off the \"expenses\" column from these datasets to create new datasets called `train_labels` and `test_labels`. Use these labels when training your model.\n",
        "\n",
        "Create a model and train it with the `train_dataset`. Run the final cell in this notebook to check your model. The final cell will use the unseen `test_dataset` to check how well the model generalizes.\n",
        "\n",
        "To pass the challenge, `model.evaluate` must return a Mean Absolute Error of under 3500. This means it predicts health care costs correctly within $3500.\n",
        "\n",
        "The final cell will also predict expenses using the `test_dataset` and graph the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rRo8oNqZ-Rj"
      },
      "source": [
        "# Import libraries. You may or may not use all of these.\n",
        "!pip install -q git+https://github.com/tensorflow/docs\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split # This is for splitting the dataset into training and testing\n",
        "\n",
        "# Most of my answers are gotten from the tensorflow website\n",
        "# https://www.tensorflow.org/tutorials/keras/regression#one_variable\n",
        "# There are many many ways to use a regression model. You can use pure tensorflow (Algo 1), you can use sklearn, but in this example we will use keras instead.\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.plots\n",
        "import tensorflow_docs.modeling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiX2FI4gZtTt"
      },
      "source": [
        "# Import data\n",
        "!wget https://cdn.freecodecamp.org/project-data/health-costs/insurance.csv\n",
        "dataset = pd.read_csv('insurance.csv')\n",
        "dataset.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcopvQh3X-kX"
      },
      "source": [
        "dataset.info()\n",
        "# No null values\n",
        "# 4 numerical (age, bmi, no. of children, expenses) and 3 categorical (sex, smoker, region)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvMMjnktdJhZ"
      },
      "source": [
        "sns.pairplot(dataset, hue = 'region') # It is good to do a pairplot to see the distribution of the data, as you may see some obvious patterns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o85vFjQIuwmf"
      },
      "source": [
        "# We should first convert the categorical columns into integers (sex, smoker region)\n",
        "# There are several ways to do this: Use this website https://www.tensorflow.org/tutorials/keras/regression#one_variable\n",
        "# Note that if we use the tensorflow way to do this machine learning (Algo 1), we have to convert the categorical columns using the tf.feature_column method\n",
        "# However, since we are using keras, the keras model input allows for panda dataframe\n",
        "\n",
        "dataset = pd.get_dummies(dataset, columns=['sex', 'smoker', 'region'], prefix='', prefix_sep='') \n",
        "# This get_dummies method takes all the unique values out and forms a new column in the dataset\n",
        "\n",
        "dataset.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3O6A3D4pYUC"
      },
      "source": [
        "# Next, let's split the dataset into training and testing 80:20 ratio\n",
        "# Then we separate the dependent variable from the independent variables (i.e. expenses)\n",
        "\n",
        "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2)\n",
        "\n",
        "train_labels = train_dataset.pop('expenses')\n",
        "test_labels = test_dataset.pop('expenses')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cCvmoPrxZ4W"
      },
      "source": [
        "# We will start building and training the model (For this cell we shall go with the multiple linear regression model)\n",
        "\n",
        "# It is good practice to normalize the dataset that uses features with different scales and ranges\n",
        "# This is because the features are multiplied by the model weights, so the scale of the outputs are affected by the scales of the inputs\n",
        "normalizer = preprocessing.Normalization()\n",
        "\n",
        "# Build the model with the normalizer layer and dense layer\n",
        "model = tf.keras.Sequential([\n",
        "    normalizer,\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss=['mean_absolute_error', 'mean_squared_error'],\n",
        "    metrics = ['mae', 'mse'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset, train_labels, \n",
        "    epochs=200,\n",
        "    # suppress logging (meaning you cannot see the training progress of the model)\n",
        "    verbose=0,\n",
        "    # Calculate validation results on 20% of the training data\n",
        "    validation_split = 0.2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Eqiz2DB5VjA"
      },
      "source": [
        "# We will start building and training the model (For this cell we shall go with the Deep Neural Network (DNN) with linear regression layer model)\n",
        "# This section implements single-input and multiple-input DNN models. The code is basically the same except the model is expanded to include some \"hidden\" non-linear layers. \n",
        "# The name \"hidden\" here just means not directly connected to the inputs or outputs.\n",
        "# These models will contain a few more layers than the linear model:\n",
        "# The normalization layer.\n",
        "# Two hidden, nonlinear, Dense layers using the relu nonlinearity.\n",
        "# A linear single-output layer.\n",
        "\n",
        "\n",
        "# Normalize the training dataset\n",
        "normalizer = preprocessing.Normalization()\n",
        "\n",
        "# Define the function to build and compile the model with the normalizer layer and dense layer\n",
        "def build_and_compile_model(norm):\n",
        "  model = keras.Sequential([\n",
        "      norm,\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "      loss=['mean_absolute_error', 'mean_squared_error'],\n",
        "      metrics = ['mae', 'mse'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "model = build_and_compile_model(normalizer)\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset, train_labels, \n",
        "    epochs=150,\n",
        "    # suppress logging (meaning you cannot see the training progress of the model)\n",
        "    verbose=0,\n",
        "    # Calculate validation results on 20% of the training data\n",
        "    validation_split = 0.2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OemjgKiI59Jm"
      },
      "source": [
        "# Visualize model's training progress using the data stored in the history object (from model.fit)\n",
        "\n",
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.ylim([0, 10000])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error [expenses]')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "plot_loss(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe7RXH3N3CWU"
      },
      "source": [
        "# RUN THIS CELL TO TEST YOUR MODEL. DO NOT MODIFY CONTENTS.\n",
        "# Test model by checking how well the model generalizes using the test set.\n",
        "loss, mae, mse = model.evaluate(test_dataset, test_labels, verbose=2)\n",
        "\n",
        "print(\"Testing set Mean Abs Error: {:5.2f} expenses\".format(mae))\n",
        "\n",
        "if mae < 3500:\n",
        "  print(\"You passed the challenge. Great job!\")\n",
        "else:\n",
        "  print(\"The Mean Abs Error must be less than 3500. Keep trying.\")\n",
        "\n",
        "# Plot predictions.\n",
        "test_predictions = model.predict(test_dataset).flatten()\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True values (expenses)')\n",
        "plt.ylabel('Predictions (expenses)')\n",
        "lims = [0, 50000]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims,lims)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuR3YyDi1Mn-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}